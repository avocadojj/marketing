{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 17:42:10 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.100.53 instead (on interface wlp3s0)\n",
      "23/07/21 17:42:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/21 17:42:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import os\n",
    "\n",
    "# Create spark session\n",
    "spark = (SparkSession\n",
    "    .builder \n",
    "    .appName(\"spark-cleansing\") \n",
    "    .getOrCreate()\n",
    "    )\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5AXe5v0tJjR1",
    "outputId": "d09a923b-a133-4073-976c-847f3c30d6bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# path file\n",
    "####################################\n",
    "csv_file = \"/home/azril/finproj/datasets/bank_marketing.csv\"\n",
    "\n",
    "####################################\n",
    "# Read csv Data\n",
    "####################################\n",
    "#from csv to parquet\n",
    "\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"sep\", \";\")\n",
    "    .option(\"header\", True)\n",
    "    .load(csv_file)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPvrn-DJ8Bcr",
    "outputId": "fd16b54e-3ad6-44b3-f2e3-9900e39e3ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Data: (41188, 21)\n",
      "Rows of Data: 41188\n",
      "Columns of Data: 21\n"
     ]
    }
   ],
   "source": [
    "rows = df.count()\n",
    "cols = len(df.columns)\n",
    "\n",
    "print(f'Dimensions of Data: {(rows,cols)}')\n",
    "print(f'Rows of Data: {rows}')\n",
    "print(f'Columns of Data: {cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AIkQIYX4gRh",
    "outputId": "aa89f52b-359d-4e21-d8f9-50b2227b984f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "|age|      job|marital|  education|default|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|  y|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "| 56|housemaid|married|   basic.4y|     no|     no|  no|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 57| services|married|high.school|unknown|     no|  no|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 37| services|married|high.school|     no|    yes|  no|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 40|   admin.|married|   basic.6y|     no|     no|  no|telephone|  may|        mon|     151|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 56| services|married|high.school|     no|     no| yes|telephone|  may|        mon|     307|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform1 = df.withColumn(\"education\",\n",
    "                                        when(df.education.endswith('4y'), regexp_replace(df.education, 'basic.4y', 'basic')) \\\n",
    "                                        .when(df.education.endswith('6y'), regexp_replace(df.education, 'basic.6y', 'basic')) \\\n",
    "                                         .when(df.education.endswith('9y'), regexp_replace(df.education, 'basic.9y', 'basic')) \\\n",
    "                                         .otherwise(df.education)\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "|age|      job|marital|  education|default|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|  y|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "| 56|housemaid|married|      basic|     no|     no|  no|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 57| services|married|high.school|unknown|     no|  no|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 37| services|married|high.school|     no|    yes|  no|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 40|   admin.|married|      basic|     no|     no|  no|telephone|  may|        mon|     151|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "| 56| services|married|high.school|     no|     no| yes|telephone|  may|        mon|     307|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191| no|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transform1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Some column name need to be standardized because Spark & BigQuery can't read it\n",
    "df_transform2 = df_transform1.withColumnRenamed('emp.var.rate', 'emp_var_rate') \\\n",
    "       .withColumnRenamed('cons.price.idx', 'cons_price_idx') \\\n",
    "       .withColumnRenamed('cons.conf.idx', 'cons_conf_idx') \\\n",
    "       .withColumnRenamed('nr.employed', 'nr_employed') \\\n",
    "       .withColumnRenamed('default', 'credit') \\\n",
    "       .withColumnRenamed('y', 'subcribed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---------+\n",
      "|age|      job|marital|  education| credit|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp_var_rate|cons_price_idx|cons_conf_idx|euribor3m|nr_employed|subcribed|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---------+\n",
      "| 56|housemaid|married|      basic|     no|     no|  no|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|\n",
      "| 57| services|married|high.school|unknown|     no|  no|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|\n",
      "| 37| services|married|high.school|     no|    yes|  no|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|\n",
      "| 40|   admin.|married|      basic|     no|     no|  no|telephone|  may|        mon|     151|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|\n",
      "| 56| services|married|high.school|     no|     no| yes|telephone|  may|        mon|     307|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|\n",
      "+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transform2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform3 = df_transform2.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of Data: (41188, 21)\n",
      "Rows of Data: 41188\n",
      "Columns of Data: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rows = df_transform3.count()\n",
    "cols = len(df_transform3.columns)\n",
    "\n",
    "print(f'Dimensions of Data: {(rows,cols)}')\n",
    "print(f'Rows of Data: {rows}')\n",
    "print(f'Columns of Data: {cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---------+\n",
      "|client_id|age|      job|marital|  education| credit|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp_var_rate|cons_price_idx|cons_conf_idx|euribor3m|nr_employed|subcribed|\n",
      "+---------+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---------+\n",
      "|        0| 56|housemaid|married|      basic|     no|     no|  no|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|\n",
      "|        1| 57| services|married|high.school|unknown|     no|  no|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|\n",
      "|        2| 37| services|married|high.school|     no|    yes|  no|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|\n",
      "|        3| 40|   admin.|married|      basic|     no|     no|  no|telephone|  may|        mon|     151|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|\n",
      "|        4| 56| services|married|high.school|     no|     no| yes|telephone|  may|        mon|     307|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|\n",
      "+---------+---+---------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transform4 = df_transform3.withColumn(\"client_id\", monotonically_increasing_id())\n",
    "df_transform4 = df_transform4.select([\"client_id\"] + [col for col in df_transform4.columns if col != \"client_id\"])\n",
    "df_transform4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---------+----------+\n",
      "|client_id|age|        job| marital|          education| credit|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp_var_rate|cons_price_idx|cons_conf_idx|euribor3m|nr_employed|subcribed|      date|\n",
      "+---------+---+-----------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---------+----------+\n",
      "|        0| 56|  housemaid| married|              basic|     no|     no|  no|telephone|    5|          1|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|        1| 57|   services| married|        high.school|unknown|     no|  no|telephone|    5|          1|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|        2| 37|   services| married|        high.school|     no|    yes|  no|telephone|    5|          1|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|        3| 40|     admin.| married|              basic|     no|     no|  no|telephone|    5|          1|     151|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|        4| 56|   services| married|        high.school|     no|     no| yes|telephone|    5|          1|     307|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|        5| 45|   services| married|              basic|unknown|     no|  no|telephone|    5|          1|     198|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|        6| 59|     admin.| married|professional.course|     no|     no|  no|telephone|    5|          1|     139|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|        7| 41|blue-collar| married|            unknown|unknown|     no|  no|telephone|    5|          1|     217|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|        8| 24| technician|  single|professional.course|     no|    yes|  no|telephone|    5|          1|     380|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|        9| 25|   services|  single|        high.school|     no|    yes|  no|telephone|    5|          1|      50|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|       10| 41|blue-collar| married|            unknown|unknown|     no|  no|telephone|    5|          1|      55|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|       11| 25|   services|  single|        high.school|     no|    yes|  no|telephone|    5|          1|     222|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|       12| 29|blue-collar|  single|        high.school|     no|     no| yes|telephone|    5|          1|     137|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|       13| 57|  housemaid|divorced|              basic|     no|    yes|  no|telephone|    5|          1|     293|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|       14| 35|blue-collar| married|              basic|     no|    yes|  no|telephone|    5|          1|     146|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|       15| 54|    retired| married|              basic|unknown|    yes| yes|telephone|    5|          1|     174|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|       16| 35|blue-collar| married|              basic|     no|    yes|  no|telephone|    5|          1|     312|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|       17| 46|blue-collar| married|              basic|unknown|    yes| yes|telephone|    5|          1|     440|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|       18| 50|blue-collar| married|              basic|     no|    yes| yes|telephone|    5|          1|     353|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "|       19| 39| management|  single|              basic|unknown|     no|  no|telephone|    5|          1|     195|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|       5191|       no|2021-05-01|\n",
      "+---------+---+-----------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "import datetime\n",
    "\n",
    "# Define UDFs to map the month and day_of_week columns\n",
    "month_dict = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6, \n",
    "              'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n",
    "\n",
    "day_dict = {'mon': 1, 'tue': 2, 'wed': 3, 'thu': 4, 'fri': 5, 'sat': 6, 'sun': 7}  # We consider Monday as 1\n",
    "\n",
    "udf_month = udf(lambda x: month_dict.get(x), IntegerType())\n",
    "udf_day = udf(lambda x: day_dict.get(x), IntegerType())\n",
    "\n",
    "df_transformed5 = df_transform4.withColumn(\"month\", udf_month(col(\"month\")))\n",
    "df_transformed5 = df_transformed5.withColumn(\"day_of_week\", udf_day(col(\"day_of_week\")))\n",
    "\n",
    "# Use a UDF to combine year, month, and day into a date column\n",
    "def create_date(year, month, day_of_week):\n",
    "    if month >= 5:\n",
    "        year = 2021\n",
    "    else:\n",
    "        year = 2022\n",
    "\n",
    "    return datetime.date(year, month, day_of_week)\n",
    "\n",
    "udf_create_date = udf(create_date, DateType())\n",
    "\n",
    "df_transformed5 = df_transformed5.withColumn('date', udf_create_date(F.lit(2021), 'month', 'day_of_week'))\n",
    "\n",
    "# Show transformed data\n",
    "df_transformed5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 14:56:32 ERROR Executor: Exception in task 0.0 in stage 7.0 (TID 7)/ 1]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_79327/3061106884.py\", line 8, in calculate_date\n",
      "  File \"/home/azril/finproj/test/lib/python3.8/site-packages/pandas/core/generic.py\", line 1466, in __nonzero__\n",
      "    raise ValueError(\n",
      "ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/07/21 14:56:32 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 7) (192.168.100.53 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_79327/3061106884.py\", line 8, in calculate_date\n",
      "  File \"/home/azril/finproj/test/lib/python3.8/site-packages/pandas/core/generic.py\", line 1466, in __nonzero__\n",
      "    raise ValueError(\n",
      "ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/07/21 14:56:32 ERROR TaskSetManager: Task 0 in stage 7.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_79327/3061106884.py\", line 8, in calculate_date\n  File \"/home/azril/finproj/test/lib/python3.8/site-packages/pandas/core/generic.py\", line 1466, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_transform5\u001b[39m.\u001b[39;49mshow(\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/spark-3.4.1-bin-hadoop3/python/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark-3.4.1-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_79327/3061106884.py\", line 8, in calculate_date\n  File \"/home/azril/finproj/test/lib/python3.8/site-packages/pandas/core/generic.py\", line 1466, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n"
     ]
    }
   ],
   "source": [
    "df_transform5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_transform3_gbq = df_transform3.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/home/Archie/.google/credentials/google_credentials.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18808.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Set project_id to your Google Cloud Platform project ID.\n",
    "project_id = \"data-fellowship-9-project\"\n",
    "\n",
    "# TODO: Set dataset_id to the full destination dataset ID.\n",
    "table_id = 'final_project.raw_credit_card'\n",
    "\n",
    "pandas_gbq.to_gbq(df_transform3_gbq, table_id, project_id=project_id)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "63sBIw570wa0",
    "wCvmXWe604GH",
    "jPZyFzW71Aqj",
    "MTgeGRyV1OEQ",
    "KQBdb59i1WJ1"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "90f0f2030db970c9fc47ed8268b8e53f011e5ea9da50dda370f41a5adb932a7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
